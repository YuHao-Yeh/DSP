{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF 由兩部分組成：`詞頻`&`逆向文件頻率`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\cdot$ 文字量化(向量化)\n",
    "* One-Hot Encoding\n",
    "* Bag of Words(BoW)\n",
    "* Bag of M-Grams(BoN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `詞頻（term frequency, t）`\n",
    "\n",
    "單詞(word)出現在一份見的頻率，需要將文件內的單詞進行正規化，再除以文件的長度。\n",
    "\n",
    "$tf(t, d) := \\dfrac{f_{t,d}}{\\Sigma_{t' \\in d} f_{t',d}} = \\dfrac{(\\textit{Number of occurrences of term } t \\textit{ in document } d)}{(\\textit{Total number of terms in the document }d)}$\n",
    "\n",
    "$f_{t,d}$ 表示單詞 $t$ 在文件 $d$ 中的次數。\n",
    "\n",
    "※ 衡量單詞對於文件重要程度的定義方式並不唯一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `逆向文件頻率（Inverse Document Frequency, IDF）`\n",
    "\n",
    "有些單詞詞頻雖然很高，但不具重要性，因此需要考慮單詞對語料庫的重要程度。\n",
    "\n",
    "$idf(t, D) := \\ln{\\left(\\dfrac{N}{1 + |\\{d \\in D | t \\in d\\}|}\\right)} = \\ln{\\left(\\dfrac{\\textit{Total number of documents in the corpus}}{\\textit{Numbers of documents with term } t \\textit{ in them}}\\right)}$\n",
    "\n",
    "$D$表示語料庫，其元素為文件 $d$。\n",
    "\n",
    "> * 分母加1是為了避免因單不再語料庫中導致分母為零的狀況 $\\Rightarrow$ well-defined\n",
    "> * idf 越大：單詞越集中出現在某幾份文件中，對於整個語料庫而言就愈重要。\n",
    "> * idf 越小：單詞在大量文件中都出現，會被認為這個單詞越一般（較不重要）。\n",
    "\n",
    "※ 衡量單詞對於整個語料庫重要程度的定義方式並不唯一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "\n",
    "$tf-idf(t, d, D) := tf(t, d) \\cdot idf(t, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf Score 與文件矩陣（Term-Document Matrix）Example："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence!', 'This is my second sentence.', 'Is this my third sentence?']\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "\n",
    "# sample documents\n",
    "document_1 = \"This is the first sentence!\"\n",
    "document_2 = \"This is my second sentence.\"\n",
    "document_3 = \"Is this my third sentence?\"\n",
    "\n",
    "# corpus of documents\n",
    "corpus = [document_1, document_2, document_3]\n",
    "print(corpus)\n",
    "\n",
    "# preprocess documents\n",
    "# processed_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first sentence', 'second sentence', 'third sentence']\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from typing import List, Optional, Union, Callable\n",
    "\n",
    "# Third party libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "from names_dataset import NameDataset\n",
    "\n",
    "from text_preprocessing import to_lower, remove_url, remove_email, remove_phone_number, remove_special_character\n",
    "from text_preprocessing import remove_itemized_bullet_and_numbering, expand_contraction, remove_punctuation, remove_whitespace\n",
    "from text_preprocessing import remove_stopword, check_spelling, normalize_unicode, substitute_token\n",
    "\n",
    "def preprocess_text(input_text: str, processing_function_list: Optional[List[Callable]] = None) -> str:\n",
    "   \"\"\" Preprocess an input text by executing a series of preprocessing functions specified in functions list \"\"\"\n",
    "   if processing_function_list is None:\n",
    "      processing_function_list = [to_lower,\n",
    "                                  remove_url,\n",
    "                                  remove_email,\n",
    "                                  remove_phone_number,\n",
    "                                  remove_itemized_bullet_and_numbering,\n",
    "                                  expand_contraction,\n",
    "                                  check_spelling,\n",
    "                                  remove_special_character,\n",
    "                                  remove_punctuation,\n",
    "                                  remove_whitespace,\n",
    "                                  normalize_unicode,\n",
    "                                  remove_stopword,\n",
    "                                  substitute_token]\n",
    "   for func in processing_function_list:\n",
    "        input_text = func(input_text)\n",
    "   if isinstance(input_text, str):\n",
    "        processed_text = input_text\n",
    "   else:\n",
    "        processed_text = ' '.join(input_text)\n",
    "   return processed_text\n",
    "\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer 得到每個單詞相對於各個文件的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialise TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(norm = None)\n",
    "\n",
    "# obtain weights of each term to each document in corpus (ie, tf-idf scores)\n",
    "tf_idf_scores = vectoriser.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件矩陣（term-document matrix）用來表示各個單詞在整個語料庫中之於文件的重要性，由 tf-idf scores 所構成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          first sentence  second sentence  third sentence\n",
      "first           1.693147         0.000000        0.000000\n",
      "is              1.000000         1.000000        1.000000\n",
      "my              0.000000         1.287682        1.287682\n",
      "second          0.000000         1.693147        0.000000\n",
      "sentence        1.000000         1.000000        1.000000\n",
      "the             1.693147         0.000000        0.000000\n",
      "third           0.000000         0.000000        1.693147\n",
      "this            1.000000         1.000000        1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of terms\n",
    "feature_names = vectoriser.get_feature_names()\n",
    "corpus_index = [n for n in processed_corpus]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores: Term-Document Matrix\n",
    "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index = feature_names, columns = corpus_index)\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
